{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사진 지문자 인식 모델 (Full Body)\n",
    "\n",
    "**목표**: 사진/실시간 카메라로 지문자(ㄱ,ㄴ,ㄷ,...,ㅏ,ㅓ,ㅗ,...) 인식\n",
    "\n",
    "**키포인트 구조 (411차원 - 기존 영상 모델과 동일)**:\n",
    "- Pose: 25 landmarks * 3 = 75\n",
    "- Face: 70 landmarks * 3 = 210\n",
    "- Left Hand: 21 landmarks * 3 = 63\n",
    "- Right Hand: 21 landmarks * 3 = 63\n",
    "- **Total: 411 차원**\n",
    "\n",
    "**모델 구조**:\n",
    "1. MediaPipe 키포인트 추출 (Pose + Face + Hands) + MLP 분류\n",
    "2. CNN 이미지 분류 (EfficientNet)\n",
    "3. 앙상블 (두 모델 결합)\n",
    "4. 실시간 카메라 인식\n",
    "\n",
    "**데이터**: `/Users/garyeong/project-1/사진_지문자/` (파일명 = 라벨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 라이브러리 설치 및 임포트\n",
    "# !pip install mediapipe opencv-python torch torchvision timm pillow\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 경로 설정\n",
    "DATA_DIR = \"/Users/garyeong/project-1/사진_지문자\"\n",
    "MODEL_DIR = \"/Users/garyeong/project-1/morpheme/photo_model\"\n",
    "\n",
    "print(f\"데이터 경로: {DATA_DIR}\")\n",
    "print(f\"모델 저장 경로: {MODEL_DIR}\")\n",
    "\n",
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# 키포인트 차원 설정 (기존 영상 모델과 동일)\n",
    "POSE_DIM = 75      # 25 landmarks * 3\n",
    "FACE_DIM = 210     # 70 landmarks * 3\n",
    "HAND_DIM = 63      # 21 landmarks * 3\n",
    "TOTAL_DIM = POSE_DIM + FACE_DIM + HAND_DIM * 2  # 411\n",
    "\n",
    "print(f\"\\n키포인트 차원: {TOTAL_DIM} (Pose:{POSE_DIM} + Face:{FACE_DIM} + LHand:{HAND_DIM} + RHand:{HAND_DIM})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 데이터 로드 및 확인\n",
    "\n",
    "# 이미지 파일 목록\n",
    "image_files = glob.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
    "image_files.extend(glob.glob(os.path.join(DATA_DIR, \"*.jpg\")))\n",
    "image_files.extend(glob.glob(os.path.join(DATA_DIR, \"*.jpeg\")))\n",
    "image_files = sorted(image_files)\n",
    "\n",
    "print(f\"발견된 이미지: {len(image_files)}개\")\n",
    "\n",
    "# 라벨 추출 (파일명 = 라벨)\n",
    "labels = []\n",
    "for img_path in image_files:\n",
    "    filename = os.path.basename(img_path)\n",
    "    label = os.path.splitext(filename)[0]  # 확장자 제거\n",
    "    labels.append(label)\n",
    "\n",
    "# 라벨 통계\n",
    "unique_labels = sorted(set(labels))\n",
    "print(f\"\\n고유 라벨 수: {len(unique_labels)}개\")\n",
    "print(f\"라벨 목록: {unique_labels}\")\n",
    "\n",
    "# 라벨 → 인덱스 매핑\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"\\n클래스 수: {num_classes}\")\n",
    "print(f\"라벨 매핑: {label_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Full Body 키포인트 추출 함수 (Pose + Face + Hands = 411차원)\n",
    "\n",
    "def extract_full_keypoints(image, pose, hands, face_mesh):\n",
    "    \"\"\"\n",
    "    이미지에서 전체 키포인트 추출 (기존 영상 모델과 동일한 411차원 구조)\n",
    "    \n",
    "    Returns:\n",
    "        keypoints: (411,) - Pose(75) + Face(210) + LHand(63) + RHand(63)\n",
    "        detection_info: dict - 각 부위 감지 성공 여부\n",
    "    \"\"\"\n",
    "    # BGR -> RGB\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # MediaPipe 처리\n",
    "    pose_results = pose.process(rgb_image)\n",
    "    hands_results = hands.process(rgb_image)\n",
    "    face_results = face_mesh.process(rgb_image)\n",
    "    \n",
    "    detection_info = {'pose': False, 'face': False, 'left_hand': False, 'right_hand': False}\n",
    "    \n",
    "    # 1. Pose keypoints (25 landmarks * 3 = 75)\n",
    "    pose_kps = []\n",
    "    if pose_results.pose_landmarks:\n",
    "        detection_info['pose'] = True\n",
    "        for i in range(25):  # 처음 25개 랜드마크만 사용\n",
    "            if i < len(pose_results.pose_landmarks.landmark):\n",
    "                lm = pose_results.pose_landmarks.landmark[i]\n",
    "                pose_kps.extend([lm.x, lm.y, lm.visibility])\n",
    "            else:\n",
    "                pose_kps.extend([0.0, 0.0, 0.0])\n",
    "    else:\n",
    "        pose_kps = [0.0] * POSE_DIM\n",
    "    \n",
    "    # 2. Face keypoints (70 landmarks * 3 = 210)\n",
    "    face_kps = []\n",
    "    if face_results.multi_face_landmarks:\n",
    "        detection_info['face'] = True\n",
    "        face_landmarks = face_results.multi_face_landmarks[0]\n",
    "        for i in range(70):  # 처음 70개 랜드마크만 사용\n",
    "            if i < len(face_landmarks.landmark):\n",
    "                lm = face_landmarks.landmark[i]\n",
    "                face_kps.extend([lm.x, lm.y, lm.z])\n",
    "            else:\n",
    "                face_kps.extend([0.0, 0.0, 0.0])\n",
    "    else:\n",
    "        face_kps = [0.0] * FACE_DIM\n",
    "    \n",
    "    # 3. Hand keypoints (21 * 3 * 2 = 126)\n",
    "    left_hand_kps = [0.0] * HAND_DIM\n",
    "    right_hand_kps = [0.0] * HAND_DIM\n",
    "    \n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n",
    "            handedness = hands_results.multi_handedness[idx].classification[0].label\n",
    "            hand_kps = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                hand_kps.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # 미러링 고려: MediaPipe는 카메라 기준이므로 반대\n",
    "            if handedness == \"Left\":\n",
    "                left_hand_kps = hand_kps\n",
    "                detection_info['left_hand'] = True\n",
    "            elif handedness == \"Right\":\n",
    "                right_hand_kps = hand_kps\n",
    "                detection_info['right_hand'] = True\n",
    "    \n",
    "    # 전체 키포인트 결합 (411차원)\n",
    "    combined_kps = np.concatenate([\n",
    "        np.array(pose_kps),\n",
    "        np.array(face_kps),\n",
    "        np.array(left_hand_kps),\n",
    "        np.array(right_hand_kps)\n",
    "    ])\n",
    "    \n",
    "    return combined_kps.astype(np.float32), detection_info\n",
    "\n",
    "print(\"=== Full Body 키포인트 추출 함수 정의 완료 ===\")\n",
    "print(f\"출력 차원: {TOTAL_DIM} (Pose:{POSE_DIM} + Face:{FACE_DIM} + LHand:{HAND_DIM} + RHand:{HAND_DIM})\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: 모든 이미지에서 키포인트 추출\n\nprint(\"=== 이미지에서 키포인트 추출 시작 ===\")\n\nall_keypoints = []\nall_labels_idx = []\nall_images_for_cnn = []\ndetection_stats = {'pose': 0, 'face': 0, 'left_hand': 0, 'right_hand': 0}\n\n# MediaPipe 초기화\nwith mp_pose.Pose(\n    static_image_mode=True,\n    model_complexity=2,\n    enable_segmentation=False,\n    min_detection_confidence=0.5\n) as pose, mp_hands.Hands(\n    static_image_mode=True,\n    max_num_hands=2,\n    model_complexity=1,\n    min_detection_confidence=0.5\n) as hands, mp_face_mesh.FaceMesh(\n    static_image_mode=True,\n    max_num_faces=1,\n    refine_landmarks=True,\n    min_detection_confidence=0.5\n) as face_mesh:\n    \n    for idx, img_path in enumerate(image_files):\n        # 이미지 로드\n        image = cv2.imread(img_path)\n        if image is None:\n            print(f\"  [SKIP] 로드 실패: {img_path}\")\n            continue\n        \n        # 키포인트 추출\n        keypoints, detection_info = extract_full_keypoints(image, pose, hands, face_mesh)\n        \n        # 라벨 인덱스\n        filename = os.path.basename(img_path)\n        label = os.path.splitext(filename)[0]\n        label_idx = label_to_idx[label]\n        \n        # 저장\n        all_keypoints.append(keypoints)\n        all_labels_idx.append(label_idx)\n        all_images_for_cnn.append(img_path)\n        \n        # 통계 업데이트\n        for key in detection_stats:\n            if detection_info.get(key, False):\n                detection_stats[key] += 1\n        \n        # 진행 상황\n        if (idx + 1) % 10 == 0 or idx == len(image_files) - 1:\n            print(f\"  진행: {idx + 1}/{len(image_files)}\")\n\n# numpy 배열로 변환\nall_keypoints = np.array(all_keypoints, dtype=np.float32)\n\nprint(f\"\\n=== 키포인트 추출 완료 ===\")\nprint(f\"총 샘플: {len(all_keypoints)}개\")\nprint(f\"키포인트 shape: {all_keypoints.shape}\")\nprint(f\"\\n감지 통계:\")\nprint(f\"  - Pose: {detection_stats['pose']}/{len(all_keypoints)} ({100*detection_stats['pose']/len(all_keypoints):.1f}%)\")\nprint(f\"  - Face: {detection_stats['face']}/{len(all_keypoints)} ({100*detection_stats['face']/len(all_keypoints):.1f}%)\")\nprint(f\"  - Left Hand: {detection_stats['left_hand']}/{len(all_keypoints)} ({100*detection_stats['left_hand']/len(all_keypoints):.1f}%)\")\nprint(f\"  - Right Hand: {detection_stats['right_hand']}/{len(all_keypoints)} ({100*detection_stats['right_hand']/len(all_keypoints):.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: 데이터 증강 및 가중치 적용 (손에 0.8 가중치)\n\n# 부위별 가중치 설정\nPOSE_WEIGHT = 0.1   # 몸\nFACE_WEIGHT = 0.1   # 얼굴\nHAND_WEIGHT = 0.8   # 손 (중요!)\n\nprint(f\"=== 부위별 가중치 ===\")\nprint(f\"  - Pose (몸): {POSE_WEIGHT}\")\nprint(f\"  - Face (얼굴): {FACE_WEIGHT}\")\nprint(f\"  - Hands (손): {HAND_WEIGHT}\")\n\ndef apply_body_part_weights(keypoints):\n    \"\"\"\n    각 부위별 가중치 적용\n    - Pose: 0.1\n    - Face: 0.1  \n    - Hands: 0.8\n    \"\"\"\n    weighted_kp = keypoints.copy()\n    \n    # 부위별 인덱스 범위\n    pose_start, pose_end = 0, POSE_DIM\n    face_start, face_end = POSE_DIM, POSE_DIM + FACE_DIM\n    lhand_start, lhand_end = POSE_DIM + FACE_DIM, POSE_DIM + FACE_DIM + HAND_DIM\n    rhand_start, rhand_end = POSE_DIM + FACE_DIM + HAND_DIM, TOTAL_DIM\n    \n    # 가중치 적용\n    weighted_kp[pose_start:pose_end] *= POSE_WEIGHT\n    weighted_kp[face_start:face_end] *= FACE_WEIGHT\n    weighted_kp[lhand_start:lhand_end] *= HAND_WEIGHT\n    weighted_kp[rhand_start:rhand_end] *= HAND_WEIGHT\n    \n    return weighted_kp\n\ndef augment_keypoints_411(keypoints, num_augment=20):\n    \"\"\"\n    411차원 키포인트 데이터 증강\n    - 노이즈 추가\n    - 스케일 변화\n    - 2D 회전\n    \"\"\"\n    augmented = [keypoints.copy()]\n    \n    # 각 부위별 인덱스\n    pose_end = POSE_DIM\n    face_end = pose_end + FACE_DIM\n    lhand_end = face_end + HAND_DIM\n    rhand_end = lhand_end + HAND_DIM\n    \n    for _ in range(num_augment):\n        kp = keypoints.copy()\n        \n        # 1. 노이즈 추가 (각 부위별로)\n        noise = np.random.normal(0, 0.015, kp.shape)\n        kp += noise\n        \n        # 2. 스케일 변화 (전체)\n        scale = np.random.uniform(0.95, 1.05)\n        \n        # x, y 좌표에만 스케일 적용 (각 landmark마다 x, y, z/conf 순서)\n        for i in range(0, len(kp), 3):\n            if i + 1 < len(kp):\n                # x, y 좌표 스케일\n                center_x = 0.5\n                center_y = 0.5\n                kp[i] = (kp[i] - center_x) * scale + center_x\n                kp[i+1] = (kp[i+1] - center_y) * scale + center_y\n        \n        # 3. 2D 회전 (작은 각도)\n        angle = np.random.uniform(-10, 10) * np.pi / 180\n        cos_a, sin_a = np.cos(angle), np.sin(angle)\n        \n        for i in range(0, len(kp), 3):\n            if i + 1 < len(kp):\n                x, y = kp[i] - 0.5, kp[i+1] - 0.5\n                kp[i] = x * cos_a - y * sin_a + 0.5\n                kp[i+1] = x * sin_a + y * cos_a + 0.5\n        \n        augmented.append(kp)\n    \n    return np.array(augmented, dtype=np.float32)\n\n# 데이터 증강 적용\nprint(\"\\n=== 데이터 증강 시작 (411차원) ===\")\nNUM_AUGMENT = 30  # 각 샘플당 30개 증강 (데이터가 적으므로)\n\naugmented_keypoints = []\naugmented_labels = []\naugmented_image_paths = []\n\nfor i in range(len(all_keypoints)):\n    kp = all_keypoints[i]\n    label_idx = all_labels_idx[i]\n    img_path = all_images_for_cnn[i]\n    \n    # 1. 가중치 적용 (손에 0.8 가중치)\n    weighted_kp = apply_body_part_weights(kp)\n    \n    # 2. 원본 + 증강\n    aug_kps = augment_keypoints_411(weighted_kp, NUM_AUGMENT)\n    \n    for aug_kp in aug_kps:\n        augmented_keypoints.append(aug_kp)\n        augmented_labels.append(label_idx)\n        augmented_image_paths.append(img_path)\n\n# 텐서 변환\nX_keypoints_aug = torch.tensor(np.array(augmented_keypoints), dtype=torch.float32)\nY_labels_aug = torch.tensor(augmented_labels, dtype=torch.long)\n\nprint(f\"증강 완료!\")\nprint(f\"  - 원본: {len(all_keypoints)}개\")\nprint(f\"  - 증강 후: {len(augmented_keypoints)}개\")\nprint(f\"  - 증강 배율: {len(augmented_keypoints) // len(all_keypoints)}x\")\nprint(f\"\\nX_keypoints_aug shape: {X_keypoints_aug.shape}\")\nprint(f\"Y_labels_aug shape: {Y_labels_aug.shape}\")\nprint(f\"\\n손 가중치 0.8 적용됨!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 키포인트 정규화\n",
    "\n",
    "# 정규화 (평균 0, 표준편차 1)\n",
    "kp_mean = X_keypoints_aug.mean(dim=0, keepdim=True)\n",
    "kp_std = X_keypoints_aug.std(dim=0, keepdim=True) + 1e-8\n",
    "\n",
    "X_keypoints_norm = (X_keypoints_aug - kp_mean) / kp_std\n",
    "\n",
    "print(\"=== 키포인트 정규화 완료 (411차원) ===\")\n",
    "print(f\"정규화 전 - Mean: {X_keypoints_aug.mean():.4f}, Std: {X_keypoints_aug.std():.4f}\")\n",
    "print(f\"정규화 후 - Mean: {X_keypoints_norm.mean():.4f}, Std: {X_keypoints_norm.std():.4f}\")\n",
    "\n",
    "# 정규화 파라미터 저장 (추론 시 필요)\n",
    "norm_params = {\n",
    "    'mean': kp_mean,\n",
    "    'std': kp_std\n",
    "}\n",
    "\n",
    "print(f\"\\n정규화 파라미터 저장됨 (추론 시 사용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: 키포인트 분류 모델 (MLP - 411차원 입력)\n",
    "\n",
    "class KeypointClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Body 키포인트 기반 지문자 분류 모델\n",
    "    Input: (batch, 411) - Pose(75) + Face(210) + LHand(63) + RHand(63)\n",
    "    Output: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=411, num_classes=28, dropout=0.3):\n",
    "        super(KeypointClassifier, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: 411 -> 512\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Layer 2: 512 -> 256\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Layer 3: 256 -> 128\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Layer 4: 128 -> 64\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Output: 64 -> num_classes\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"앙상블을 위한 중간 특징 추출 (마지막 Linear 전)\"\"\"\n",
    "        for layer in list(self.model.children())[:-1]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "model_kp = KeypointClassifier(input_size=TOTAL_DIM, num_classes=num_classes, dropout=0.3)\n",
    "model_kp = model_kp.to(device)\n",
    "\n",
    "print(\"=== 키포인트 분류 모델 (411차원 입력) ===\")\n",
    "print(model_kp)\n",
    "print(f\"\\n입력 차원: {TOTAL_DIM}\")\n",
    "print(f\"출력 클래스: {num_classes}\")\n",
    "print(f\"파라미터 수: {sum(p.numel() for p in model_kp.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 키포인트 모델 학습\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset_kp = TensorDataset(X_keypoints_norm, Y_labels_aug)\n",
    "\n",
    "# Train/Val 분할 (80/20)\n",
    "train_size = int(0.8 * len(dataset_kp))\n",
    "val_size = len(dataset_kp) - train_size\n",
    "train_dataset_kp, val_dataset_kp = random_split(dataset_kp, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader_kp = DataLoader(train_dataset_kp, batch_size=batch_size, shuffle=True)\n",
    "val_loader_kp = DataLoader(val_dataset_kp, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"=== 데이터셋 분할 ===\")\n",
    "print(f\"  - 학습: {len(train_dataset_kp)}개\")\n",
    "print(f\"  - 검증: {len(val_dataset_kp)}개\")\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_kp = optim.Adam(model_kp.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler_kp = optim.lr_scheduler.ReduceLROnPlateau(optimizer_kp, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# 학습\n",
    "num_epochs = 100\n",
    "best_val_acc = 0\n",
    "\n",
    "print(f\"\\n=== 키포인트 모델 학습 시작 (411차원) ===\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 학습\n",
    "    model_kp.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader_kp:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer_kp.zero_grad()\n",
    "        outputs = model_kp(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_kp.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader_kp)\n",
    "    train_acc = 100 * train_correct / len(train_dataset_kp)\n",
    "    \n",
    "    # 검증\n",
    "    model_kp.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader_kp:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model_kp(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader_kp)\n",
    "    val_acc = 100 * val_correct / len(val_dataset_kp)\n",
    "    \n",
    "    scheduler_kp.step(val_loss)\n",
    "    \n",
    "    # 출력\n",
    "    if (epoch + 1) % 10 == 0 or val_acc > best_val_acc:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "              f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "              f\"Acc: {train_acc:.1f}%/{val_acc:.1f}%\", end=\"\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_kp.state_dict(), os.path.join(MODEL_DIR, 'model_keypoint_411_best.pth'))\n",
    "            print(f\" <- BEST\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "print(f\"\\n=== 학습 완료 ===\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: CNN 이미지 분류 모델 (EfficientNet)\n",
    "import timm\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    이미지 기반 지문자 분류 모델 (EfficientNet-B0)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=28, pretrained=True):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        \n",
    "        # EfficientNet-B0 백본\n",
    "        self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0)\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"앙상블을 위한 특징 추출\"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "# 모델 생성\n",
    "model_cnn = ImageClassifier(num_classes=num_classes, pretrained=True)\n",
    "model_cnn = model_cnn.to(device)\n",
    "\n",
    "print(\"=== CNN 이미지 분류 모델 ===\")\n",
    "print(f\"백본: EfficientNet-B0\")\n",
    "print(f\"파라미터 수: {sum(p.numel() for p in model_cnn.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: CNN 데이터셋 정의 및 학습\n",
    "\n",
    "class FingerAlphabetImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    지문자 이미지 데이터셋 (CNN용)\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# 이미지 전처리\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 원본 데이터만 사용\n",
    "dataset_cnn = FingerAlphabetImageDataset(all_images_for_cnn, all_labels_idx, transform=train_transform)\n",
    "\n",
    "# Train/Val 분할\n",
    "train_size = int(0.8 * len(dataset_cnn))\n",
    "val_size = len(dataset_cnn) - train_size\n",
    "train_dataset_cnn, val_dataset_cnn = random_split(dataset_cnn, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=8, shuffle=True)\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"=== CNN 데이터셋 ===\")\n",
    "print(f\"  - 전체: {len(dataset_cnn)}개\")\n",
    "print(f\"  - 학습: {len(train_dataset_cnn)}개\")\n",
    "print(f\"  - 검증: {len(val_dataset_cnn)}개\")\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "scheduler_cnn = optim.lr_scheduler.ReduceLROnPlateau(optimizer_cnn, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# 학습\n",
    "num_epochs_cnn = 50\n",
    "best_val_acc_cnn = 0\n",
    "\n",
    "print(f\"\\n=== CNN 모델 학습 시작 ===\")\n",
    "print(f\"Epochs: {num_epochs_cnn}\")\n",
    "\n",
    "for epoch in range(num_epochs_cnn):\n",
    "    # 학습\n",
    "    model_cnn.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader_cnn:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs = model_cnn(batch_x)\n",
    "        loss = criterion_cnn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader_cnn)\n",
    "    train_acc = 100 * train_correct / len(train_dataset_cnn)\n",
    "    \n",
    "    # 검증\n",
    "    model_cnn.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader_cnn:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model_cnn(batch_x)\n",
    "            loss = criterion_cnn(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader_cnn)\n",
    "    val_acc = 100 * val_correct / len(val_dataset_cnn)\n",
    "    \n",
    "    scheduler_cnn.step(val_loss)\n",
    "    \n",
    "    # 출력\n",
    "    if (epoch + 1) % 5 == 0 or val_acc > best_val_acc_cnn:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs_cnn} | \"\n",
    "              f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "              f\"Acc: {train_acc:.1f}%/{val_acc:.1f}%\", end=\"\")\n",
    "        \n",
    "        if val_acc > best_val_acc_cnn:\n",
    "            best_val_acc_cnn = val_acc\n",
    "            torch.save(model_cnn.state_dict(), os.path.join(MODEL_DIR, 'model_cnn_best.pth'))\n",
    "            print(f\" <- BEST\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "print(f\"\\n=== CNN 학습 완료 ===\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc_cnn:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: 앙상블 모델 정의\n",
    "\n",
    "class EnsembleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    키포인트(411차원) + CNN 앙상블 모델\n",
    "    \"\"\"\n",
    "    def __init__(self, model_kp, model_cnn, num_classes, kp_weight=0.4, cnn_weight=0.6):\n",
    "        super(EnsembleClassifier, self).__init__()\n",
    "        self.model_kp = model_kp\n",
    "        self.model_cnn = model_cnn\n",
    "        self.kp_weight = kp_weight\n",
    "        self.cnn_weight = cnn_weight\n",
    "    \n",
    "    def forward(self, keypoints, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keypoints: (batch, 411) - 정규화된 키포인트\n",
    "            image: (batch, 3, 224, 224) - 정규화된 이미지\n",
    "        Returns:\n",
    "            ensemble_probs: (batch, num_classes) - 앙상블 확률\n",
    "        \"\"\"\n",
    "        # 각 모델의 예측\n",
    "        logits_kp = self.model_kp(keypoints)\n",
    "        logits_cnn = self.model_cnn(image)\n",
    "        \n",
    "        # Softmax 확률로 변환\n",
    "        probs_kp = torch.softmax(logits_kp, dim=1)\n",
    "        probs_cnn = torch.softmax(logits_cnn, dim=1)\n",
    "        \n",
    "        # 가중 평균 앙상블\n",
    "        ensemble_probs = self.kp_weight * probs_kp + self.cnn_weight * probs_cnn\n",
    "        \n",
    "        return ensemble_probs\n",
    "\n",
    "# Best 모델 로드\n",
    "model_kp.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'model_keypoint_411_best.pth'), map_location=device))\n",
    "model_cnn.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'model_cnn_best.pth'), map_location=device))\n",
    "\n",
    "# 앙상블 모델 생성\n",
    "model_ensemble = EnsembleClassifier(model_kp, model_cnn, num_classes)\n",
    "model_ensemble = model_ensemble.to(device)\n",
    "model_ensemble.eval()\n",
    "\n",
    "print(\"=== 앙상블 모델 생성 완료 ===\")\n",
    "print(f\"키포인트 모델 가중치: {model_ensemble.kp_weight}\")\n",
    "print(f\"CNN 모델 가중치: {model_ensemble.cnn_weight}\")\n",
    "print(f\"\\n키포인트 입력: 411차원 (Pose + Face + Hands)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12: 실시간 인식 함수 정의 (Full Body + 손 가중치 0.8)\n\nclass RealTimeFingerAlphabetRecognizer:\n    \"\"\"\n    실시간 지문자 인식기 (Pose + Face + Hands = 411차원)\n    손에 0.8 가중치 적용\n    \"\"\"\n    def __init__(self, model_kp, model_cnn, model_ensemble, norm_params, label_to_idx, idx_to_label, device):\n        self.model_kp = model_kp\n        self.model_cnn = model_cnn\n        self.model_ensemble = model_ensemble\n        self.norm_params = norm_params\n        self.label_to_idx = label_to_idx\n        self.idx_to_label = idx_to_label\n        self.device = device\n        \n        # MediaPipe 초기화 (Pose + Face + Hands)\n        self.pose = mp_pose.Pose(\n            static_image_mode=False,\n            model_complexity=1,\n            enable_segmentation=False,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n        self.hands = mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            model_complexity=1,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n        self.face_mesh = mp_face_mesh.FaceMesh(\n            static_image_mode=False,\n            max_num_faces=1,\n            refine_landmarks=True,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        )\n        \n        # 이미지 전처리\n        self.image_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # 예측 안정화를 위한 히스토리\n        self.prediction_history = []\n        self.history_size = 5\n    \n    def extract_keypoints(self, frame):\n        \"\"\"프레임에서 Full Body 키포인트 추출 (411차원) + 손 가중치 0.8 적용\"\"\"\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        # MediaPipe 처리\n        pose_results = self.pose.process(rgb_frame)\n        hands_results = self.hands.process(rgb_frame)\n        face_results = self.face_mesh.process(rgb_frame)\n        \n        landmarks_info = {\n            'pose': pose_results.pose_landmarks,\n            'hands': hands_results.multi_hand_landmarks,\n            'handedness': hands_results.multi_handedness,\n            'face': face_results.multi_face_landmarks\n        }\n        \n        # 1. Pose keypoints (25 * 3 = 75)\n        pose_kps = []\n        if pose_results.pose_landmarks:\n            for i in range(25):\n                if i < len(pose_results.pose_landmarks.landmark):\n                    lm = pose_results.pose_landmarks.landmark[i]\n                    pose_kps.extend([lm.x, lm.y, lm.visibility])\n                else:\n                    pose_kps.extend([0.0, 0.0, 0.0])\n        else:\n            pose_kps = [0.0] * POSE_DIM\n        \n        # 2. Face keypoints (70 * 3 = 210)\n        face_kps = []\n        if face_results.multi_face_landmarks:\n            face_landmarks = face_results.multi_face_landmarks[0]\n            for i in range(70):\n                if i < len(face_landmarks.landmark):\n                    lm = face_landmarks.landmark[i]\n                    face_kps.extend([lm.x, lm.y, lm.z])\n                else:\n                    face_kps.extend([0.0, 0.0, 0.0])\n        else:\n            face_kps = [0.0] * FACE_DIM\n        \n        # 3. Hand keypoints (21 * 3 * 2 = 126)\n        left_hand_kps = [0.0] * HAND_DIM\n        right_hand_kps = [0.0] * HAND_DIM\n        \n        if hands_results.multi_hand_landmarks:\n            for idx, hand_landmarks in enumerate(hands_results.multi_hand_landmarks):\n                handedness = hands_results.multi_handedness[idx].classification[0].label\n                hand_kps = []\n                for lm in hand_landmarks.landmark:\n                    hand_kps.extend([lm.x, lm.y, lm.z])\n                \n                if handedness == \"Left\":\n                    left_hand_kps = hand_kps\n                elif handedness == \"Right\":\n                    right_hand_kps = hand_kps\n        \n        # 전체 키포인트 결합 (411차원)\n        combined_kps = np.concatenate([\n            np.array(pose_kps),\n            np.array(face_kps),\n            np.array(left_hand_kps),\n            np.array(right_hand_kps)\n        ]).astype(np.float32)\n        \n        # 부위별 가중치 적용 (손에 0.8 가중치)\n        weighted_kps = apply_body_part_weights(combined_kps)\n        \n        return weighted_kps, landmarks_info\n    \n    def predict(self, frame, use_ensemble=True):\n        \"\"\"\n        단일 프레임에서 지문자 예측\n        \"\"\"\n        # 키포인트 추출 (가중치 적용됨)\n        keypoints, landmarks_info = self.extract_keypoints(frame)\n        \n        # 손이 감지되지 않으면 예측 불가\n        if landmarks_info['hands'] is None:\n            return None, 0.0, landmarks_info\n        \n        # 키포인트 정규화\n        kp_tensor = torch.tensor(keypoints, dtype=torch.float32).unsqueeze(0)\n        kp_norm = (kp_tensor - self.norm_params['mean']) / self.norm_params['std']\n        kp_norm = kp_norm.to(self.device)\n        \n        if use_ensemble:\n            # 이미지 전처리\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            image_tensor = self.image_transform(rgb_frame).unsqueeze(0).to(self.device)\n            \n            # 앙상블 예측\n            with torch.no_grad():\n                probs = self.model_ensemble(kp_norm, image_tensor)\n        else:\n            # 키포인트만 사용\n            with torch.no_grad():\n                logits = self.model_kp(kp_norm)\n                probs = torch.softmax(logits, dim=1)\n        \n        # 예측 결과\n        confidence, predicted_idx = torch.max(probs, dim=1)\n        predicted_label = self.idx_to_label[predicted_idx.item()]\n        confidence = confidence.item()\n        \n        # 히스토리 기반 안정화\n        self.prediction_history.append(predicted_label)\n        if len(self.prediction_history) > self.history_size:\n            self.prediction_history.pop(0)\n        \n        if len(self.prediction_history) >= 3:\n            most_common = Counter(self.prediction_history).most_common(1)[0][0]\n            predicted_label = most_common\n        \n        return predicted_label, confidence, landmarks_info\n    \n    def draw_results(self, frame, predicted_label, confidence, landmarks_info):\n        \"\"\"결과 시각화 (Pose + Face + Hands)\"\"\"\n        # Pose 그리기\n        if landmarks_info['pose']:\n            mp_drawing.draw_landmarks(\n                frame, landmarks_info['pose'], mp_pose.POSE_CONNECTIONS,\n                mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n                mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=2)\n            )\n        \n        # Face 그리기 (간소화된 테두리)\n        if landmarks_info['face']:\n            for face_landmarks in landmarks_info['face']:\n                mp_drawing.draw_landmarks(\n                    frame, face_landmarks,\n                    mp_face_mesh.FACEMESH_CONTOURS,\n                    landmark_drawing_spec=None,\n                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1)\n                )\n        \n        # Hands 그리기\n        if landmarks_info['hands']:\n            for hand_landmarks in landmarks_info['hands']:\n                mp_drawing.draw_landmarks(\n                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=3),\n                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2)\n                )\n        \n        # 예측 결과 표시\n        if predicted_label:\n            text = f\"{predicted_label} ({confidence*100:.1f}%)\"\n            cv2.putText(frame, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n                       1.5, (0, 255, 0), 3)\n        else:\n            cv2.putText(frame, \"No hand detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX,\n                       1, (0, 0, 255), 2)\n        \n        return frame\n    \n    def release(self):\n        \"\"\"리소스 해제\"\"\"\n        self.pose.close()\n        self.hands.close()\n        self.face_mesh.close()\n\nprint(\"=== 실시간 인식 클래스 정의 완료 (Full Body) ===\")\nprint(f\"키포인트: 411차원 (Pose + Face + Hands)\")\nprint(f\"손 가중치: {HAND_WEIGHT} (Pose/Face: {POSE_WEIGHT}/{FACE_WEIGHT})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: 단일 이미지 테스트\n",
    "\n",
    "def predict_single_image(image_path, use_ensemble=True):\n",
    "    \"\"\"\n",
    "    단일 이미지에서 지문자 예측 (Full Body)\n",
    "    \"\"\"\n",
    "    # 이미지 로드\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"이미지를 로드할 수 없습니다: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 인식기 초기화\n",
    "    recognizer = RealTimeFingerAlphabetRecognizer(\n",
    "        model_kp=model_kp,\n",
    "        model_cnn=model_cnn,\n",
    "        model_ensemble=model_ensemble,\n",
    "        norm_params=norm_params,\n",
    "        label_to_idx=label_to_idx,\n",
    "        idx_to_label=idx_to_label,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 예측\n",
    "    predicted_label, confidence, landmarks_info = recognizer.predict(frame, use_ensemble)\n",
    "    \n",
    "    # 결과 출력\n",
    "    true_label = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    print(f\"\\n=== 예측 결과 (411차원) ===\")\n",
    "    print(f\"파일: {os.path.basename(image_path)}\")\n",
    "    print(f\"정답: {true_label}\")\n",
    "    print(f\"예측: {predicted_label}\")\n",
    "    print(f\"신뢰도: {confidence*100:.1f}%\")\n",
    "    print(f\"결과: {'정답' if true_label == predicted_label else '오답'}\")\n",
    "    print(f\"\\n감지 정보:\")\n",
    "    print(f\"  - Pose: {'O' if landmarks_info['pose'] else 'X'}\")\n",
    "    print(f\"  - Face: {'O' if landmarks_info['face'] else 'X'}\")\n",
    "    print(f\"  - Hands: {'O' if landmarks_info['hands'] else 'X'}\")\n",
    "    \n",
    "    recognizer.release()\n",
    "    return predicted_label, confidence\n",
    "\n",
    "# 테스트\n",
    "test_image = image_files[0] if image_files else None\n",
    "if test_image:\n",
    "    predict_single_image(test_image, use_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: 모델 저장 (최종)\n",
    "\n",
    "# 전체 모델 및 설정 저장\n",
    "save_data = {\n",
    "    # 모델 가중치\n",
    "    'model_kp_state': model_kp.state_dict(),\n",
    "    'model_cnn_state': model_cnn.state_dict(),\n",
    "    \n",
    "    # 정규화 파라미터\n",
    "    'norm_params': norm_params,\n",
    "    \n",
    "    # 라벨 매핑\n",
    "    'label_to_idx': label_to_idx,\n",
    "    'idx_to_label': idx_to_label,\n",
    "    'num_classes': num_classes,\n",
    "    \n",
    "    # 모델 설정\n",
    "    'kp_input_size': TOTAL_DIM,  # 411\n",
    "    'ensemble_weights': {'kp': 0.4, 'cnn': 0.6},\n",
    "    \n",
    "    # 키포인트 구조 정보\n",
    "    'keypoint_structure': {\n",
    "        'pose_dim': POSE_DIM,\n",
    "        'face_dim': FACE_DIM,\n",
    "        'hand_dim': HAND_DIM,\n",
    "        'total_dim': TOTAL_DIM\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(save_data, os.path.join(MODEL_DIR, 'photo_finger_alphabet_411_model.pt'))\n",
    "\n",
    "print(\"=== 모델 저장 완료 ===\")\n",
    "print(f\"저장 위치: {MODEL_DIR}/photo_finger_alphabet_411_model.pt\")\n",
    "print(f\"\\n포함 내용:\")\n",
    "print(f\"  - 키포인트 분류 모델 (MLP, 411차원 입력)\")\n",
    "print(f\"  - CNN 이미지 분류 모델 (EfficientNet-B0)\")\n",
    "print(f\"  - 정규화 파라미터\")\n",
    "print(f\"  - 라벨 매핑 ({num_classes}개 클래스)\")\n",
    "print(f\"\\n키포인트 구조:\")\n",
    "print(f\"  - Pose: {POSE_DIM} (25 landmarks * 3)\")\n",
    "print(f\"  - Face: {FACE_DIM} (70 landmarks * 3)\")\n",
    "print(f\"  - Left Hand: {HAND_DIM} (21 landmarks * 3)\")\n",
    "print(f\"  - Right Hand: {HAND_DIM} (21 landmarks * 3)\")\n",
    "print(f\"  - Total: {TOTAL_DIM}차원\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}